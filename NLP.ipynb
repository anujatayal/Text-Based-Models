{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beans ',\n",
       " 'i was trying to explain to somebody as we were flying in that s corn ',\n",
       " 'that s beans ',\n",
       " 'and they were very impressed at my agricultural knowledge ',\n",
       " 'please give it up for amaury once again for that outstanding introduction ',\n",
       " 'i have a bunch of good friends here today including somebody who i served with who is one of the finest senators in the country and we re lucky to have him your senator dick durbin is here ',\n",
       " 'i also noticed by the way former governor edgar here who i haven t seen in a long time and somehow he has not aged and i have ',\n",
       " 'and it s great to see you governor ',\n",
       " 'i want to thank president killeen and everybody at the u of i system for making it possible for me to be here today ',\n",
       " 'and i am deeply honored at the paul douglas award that is being given to me ',\n",
       " 'he is somebody who set the path for so much outstanding public service here in illinois ',\n",
       " 'now i want to start by addressing the elephant in the room ',\n",
       " 'i know people are still wondering why i didn t speak at the commencement ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Beans. I was trying to explain to somebody as we were flying in, that’s corn. That’s beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction. I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and we’re lucky to have him, your Senator, Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven’t seen in a long time, and somehow he has not aged and I have. And it’s great to see you, Governor. I want to thank President Killeen and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. He is somebody who set the path for so much outstanding public service here in Illinois. Now, I want to start by addressing the elephant in the room. I know people are still wondering why I didn’t speak at the commencement.\"\n",
    "dataset=nltk.sent_tokenize(text)\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i]=dataset[i].lower()\n",
    "    dataset[i]=re.sub(r'\\W',' ',dataset[i])\n",
    "    dataset[i]=re.sub(r'\\s+',' ',dataset[i])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beans': 2,\n",
       " 'i': 12,\n",
       " 'was': 1,\n",
       " 'trying': 1,\n",
       " 'to': 8,\n",
       " 'explain': 1,\n",
       " 'somebody': 3,\n",
       " 'as': 1,\n",
       " 'we': 2,\n",
       " 'were': 2,\n",
       " 'flying': 1,\n",
       " 'in': 5,\n",
       " 'that': 4,\n",
       " 's': 3,\n",
       " 'corn': 1,\n",
       " 'and': 7,\n",
       " 'they': 1,\n",
       " 'very': 1,\n",
       " 'impressed': 1,\n",
       " 'at': 4,\n",
       " 'my': 1,\n",
       " 'agricultural': 1,\n",
       " 'knowledge': 1,\n",
       " 'please': 1,\n",
       " 'give': 1,\n",
       " 'it': 3,\n",
       " 'up': 1,\n",
       " 'for': 5,\n",
       " 'amaury': 1,\n",
       " 'once': 1,\n",
       " 'again': 1,\n",
       " 'outstanding': 2,\n",
       " 'introduction': 1,\n",
       " 'have': 3,\n",
       " 'a': 2,\n",
       " 'bunch': 1,\n",
       " 'of': 3,\n",
       " 'good': 1,\n",
       " 'friends': 1,\n",
       " 'here': 5,\n",
       " 'today': 2,\n",
       " 'including': 1,\n",
       " 'who': 4,\n",
       " 'served': 1,\n",
       " 'with': 1,\n",
       " 'is': 4,\n",
       " 'one': 1,\n",
       " 'the': 9,\n",
       " 'finest': 1,\n",
       " 'senators': 1,\n",
       " 'country': 1,\n",
       " 're': 1,\n",
       " 'lucky': 1,\n",
       " 'him': 1,\n",
       " 'your': 1,\n",
       " 'senator': 1,\n",
       " 'dick': 1,\n",
       " 'durbin': 1,\n",
       " 'also': 1,\n",
       " 'noticed': 1,\n",
       " 'by': 2,\n",
       " 'way': 1,\n",
       " 'former': 1,\n",
       " 'governor': 2,\n",
       " 'edgar': 1,\n",
       " 'haven': 1,\n",
       " 't': 2,\n",
       " 'seen': 1,\n",
       " 'long': 1,\n",
       " 'time': 1,\n",
       " 'somehow': 1,\n",
       " 'he': 2,\n",
       " 'has': 1,\n",
       " 'not': 1,\n",
       " 'aged': 1,\n",
       " 'great': 1,\n",
       " 'see': 1,\n",
       " 'you': 1,\n",
       " 'want': 2,\n",
       " 'thank': 1,\n",
       " 'president': 1,\n",
       " 'killeen': 1,\n",
       " 'everybody': 1,\n",
       " 'u': 1,\n",
       " 'system': 1,\n",
       " 'making': 1,\n",
       " 'possible': 1,\n",
       " 'me': 2,\n",
       " 'be': 1,\n",
       " 'am': 1,\n",
       " 'deeply': 1,\n",
       " 'honored': 1,\n",
       " 'paul': 1,\n",
       " 'douglas': 1,\n",
       " 'award': 1,\n",
       " 'being': 1,\n",
       " 'given': 1,\n",
       " 'set': 1,\n",
       " 'path': 1,\n",
       " 'so': 1,\n",
       " 'much': 1,\n",
       " 'public': 1,\n",
       " 'service': 1,\n",
       " 'illinois': 1,\n",
       " 'now': 1,\n",
       " 'start': 1,\n",
       " 'addressing': 1,\n",
       " 'elephant': 1,\n",
       " 'room': 1,\n",
       " 'know': 1,\n",
       " 'people': 1,\n",
       " 'are': 1,\n",
       " 'still': 1,\n",
       " 'wondering': 1,\n",
       " 'why': 1,\n",
       " 'didn': 1,\n",
       " 'speak': 1,\n",
       " 'commencement': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2count={}\n",
    "for data in dataset:\n",
    "    words=nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word]=1\n",
    "        else:\n",
    "            word2count[word]+=1\n",
    "word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dict.get>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2count)\n",
    "word2count.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'the',\n",
       " 'to',\n",
       " 'and',\n",
       " 'in',\n",
       " 'for',\n",
       " 'here',\n",
       " 'that',\n",
       " 'at',\n",
       " 'who',\n",
       " 'is',\n",
       " 'somebody',\n",
       " 's',\n",
       " 'it',\n",
       " 'have',\n",
       " 'of',\n",
       " 'beans',\n",
       " 'we',\n",
       " 'were',\n",
       " 'outstanding',\n",
       " 'a',\n",
       " 'today',\n",
       " 'by',\n",
       " 'governor',\n",
       " 't',\n",
       " 'he',\n",
       " 'want',\n",
       " 'me',\n",
       " 'was',\n",
       " 'trying',\n",
       " 'explain',\n",
       " 'as',\n",
       " 'flying',\n",
       " 'corn',\n",
       " 'they',\n",
       " 'very',\n",
       " 'impressed',\n",
       " 'my',\n",
       " 'agricultural',\n",
       " 'knowledge',\n",
       " 'please',\n",
       " 'give',\n",
       " 'up',\n",
       " 'amaury',\n",
       " 'once',\n",
       " 'again',\n",
       " 'introduction',\n",
       " 'bunch',\n",
       " 'good',\n",
       " 'friends',\n",
       " 'including',\n",
       " 'served',\n",
       " 'with',\n",
       " 'one',\n",
       " 'finest',\n",
       " 'senators',\n",
       " 'country',\n",
       " 're',\n",
       " 'lucky',\n",
       " 'him',\n",
       " 'your',\n",
       " 'senator',\n",
       " 'dick',\n",
       " 'durbin',\n",
       " 'also',\n",
       " 'noticed',\n",
       " 'way',\n",
       " 'former',\n",
       " 'edgar',\n",
       " 'haven',\n",
       " 'seen',\n",
       " 'long',\n",
       " 'time',\n",
       " 'somehow',\n",
       " 'has',\n",
       " 'not',\n",
       " 'aged',\n",
       " 'great',\n",
       " 'see',\n",
       " 'you',\n",
       " 'thank',\n",
       " 'president',\n",
       " 'killeen',\n",
       " 'everybody',\n",
       " 'u',\n",
       " 'system',\n",
       " 'making',\n",
       " 'possible',\n",
       " 'be',\n",
       " 'am',\n",
       " 'deeply',\n",
       " 'honored',\n",
       " 'paul',\n",
       " 'douglas',\n",
       " 'award',\n",
       " 'being',\n",
       " 'given',\n",
       " 'set',\n",
       " 'path',\n",
       " 'so']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import heapq\n",
    "freq_words=heapq.nlargest(100,word2count,key=word2count.get)\n",
    "freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = [] \n",
    "for data in dataset: \n",
    "    vector = [] \n",
    "    for word in freq_words: \n",
    "        if word in nltk.word_tokenize(data): \n",
    "            vector.append(1) \n",
    "        else: \n",
    "            vector.append(0) \n",
    "    X.append(vector) \n",
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are  balls in this bag, and  in the other one.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_numbers(text):\n",
    "    result=re.sub(r'\\d+','',text)\n",
    "    return result\n",
    "text=\"There are 3 balls in this bag, and 12 in the other one.\"\n",
    "remove_numbers(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "p=inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are three balls in this bag, and twelve in the other one.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_numbers(text):\n",
    "    new_string=[]\n",
    "    words=text.split()\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            temp=p.number_to_words(word)\n",
    "            new_string.append(temp)\n",
    "        else:\n",
    "            new_string.append(word)\n",
    "    string=' '.join(new_string)\n",
    "    return string\n",
    "text=\"There are 3 balls in this bag, and 12 in the other one.\"\n",
    "replace_numbers(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There', 'EX'),\n",
       " ('are', 'VBP'),\n",
       " ('3', 'CD'),\n",
       " ('balls', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('bag,', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('12', 'CD'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('other', 'JJ'),\n",
       " ('one.', 'NN')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_tagging(text):\n",
    "    words=text.split()\n",
    "    return(nltk.pos_tag(words))\n",
    "text=\"There are 3 balls in this bag, and 12 in the other one.\"\n",
    "pos_tagging(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ bird/NN)\n",
      "  is/VBZ\n",
      "  flying/VBG\n",
      "  in/IN\n",
      "  (NP the/DT sky/NN))\n",
      "(NP the/DT little/JJ yellow/JJ bird/NN)\n",
      "(NP the/DT sky/NN)\n"
     ]
    }
   ],
   "source": [
    "def chunking(text,grammar):\n",
    "    words=text.split()\n",
    "    word_pos=nltk.pos_tag(words)\n",
    "    chunkParser=nltk.RegexpParser(grammar)\n",
    "    tree=chunkParser.parse(word_pos)\n",
    "    for subtree in tree.subtrees():\n",
    "        print(subtree)\n",
    "    tree.draw() \n",
    "      \n",
    "sentence = 'the little yellow bird is flying in the sky'\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunking(sentence, grammar)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Bill/NNP)\n",
      "  works/VBZ\n",
      "  for/IN\n",
      "  (ORGANIZATION GeeksforGeeks/NNP)\n",
      "  so/RB\n",
      "  he/PRP\n",
      "  went/VBD\n",
      "  to/TO\n",
      "  (GPE Delhi/NNP)\n",
      "  for/IN\n",
      "  a/DT\n",
      "  meetup/NN)\n"
     ]
    }
   ],
   "source": [
    "def ner(text):\n",
    "    words=text.split()\n",
    "    word_pos=nltk.pos_tag(words)\n",
    "    print(nltk.ne_chunk(word_pos))\n",
    "text='Bill works for GeeksforGeeks so he went to Delhi for a meetup'\n",
    "ner(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytext='Welcome to geeksforgeeks'\n",
    "language='en'\n",
    "obj=gTTS(text=mytext,lang=language,slow=False)\n",
    "obj.save(\"welcome.mp3\")\n",
    "os.system(\"mpg321 welcome.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:38: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.9972412\n",
      "Cosine similarity between 'alice' and 'machines' - CBOW :  0.98699635\n",
      "Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  0.92471987\n",
      "Cosine similarity between 'alice' and 'machines' - Skip Gram :  0.90657544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:47: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:51: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "# import warnings \n",
    "# warnings.filterwarnings(action = 'ignore') \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "#  Reads ‘alice.txt’ file \n",
    "sample = open(\"/Users/anujatayal/nltk_data/corpora/gutenberg/carroll-alice.txt\", \"r\") \n",
    "s = sample.read()   \n",
    "# Replaces escape character with space \n",
    "f = s.replace(\"\\n\", \" \")  \n",
    "data = [] \n",
    "  \n",
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(f): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) \n",
    "  \n",
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'alice' \" + \n",
    "               \"and 'wonderland' - CBOW : \", \n",
    "    model1.similarity('alice', 'wonderland')) \n",
    "      \n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "                 \"and 'machines' - CBOW : \", \n",
    "      model1.similarity('alice', 'machines')) \n",
    "  \n",
    "# Create Skip Gram model \n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 1) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "          \"and 'wonderland' - Skip Gram : \", \n",
    "    model2.similarity('alice', 'wonderland')) \n",
    "      \n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "            \"and 'machines' - Skip Gram : \", \n",
    "      model2.similarity('alice', 'machines')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1235b7240>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=[\"flower\",\"flow\",\"flight\"]\n",
    "len(min(s,key=len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
