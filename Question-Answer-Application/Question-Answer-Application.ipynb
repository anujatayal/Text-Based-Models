{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANUJA TAYAL\n",
    "#\n",
    "# Problem Statement:\n",
    "# Question answering system for a text article\n",
    "# \n",
    "# Method of Solution\n",
    "# - The application is designed to answer questions on an artical specifically Wikipedia using Gensim module of Doc2vec.\n",
    "# - The application is designed in form of a chatbot which greets with hi and you can answer as many questions \n",
    "#   till you thank him for his services.\n",
    "# - The application scrapes the required article using Beautiful Soup, preprocesses it to remove punctuations \n",
    "#   and other irrelevant texts. The processed text then trains the model using Doc2vec which converts every sentence of article to its corresponding vector. \n",
    "# - When the application is asked a question most probably a fact based question, question is preprocessed and \n",
    "#   trained model calculates calculates the corresponding Doc2vec vector. \n",
    "# - And the application returns the most similar sentence from the model and also the similarity.  \n",
    "#\n",
    "# Future Improvement:\n",
    "# - The Doc2vec model changes its output in every iteration \n",
    "# - The system works best for factoid questions\n",
    "# - We can use biLSTM model to train the model to increase accuracy \n",
    "# - Application can be trained to output more accurately to give the required answer.\n",
    "# - To increase the training set, we can use data from the embedded references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing text by removing links, removing anything except numbers,letters,-,. and remove extra white spaces\n",
    "def preprocess(text):\n",
    "    text=re.sub(r'\\[[0-9]*\\]','',text) #remove links\n",
    "    text=re.sub(r'[^a-zA-Z-0-9.]',' ',text) #remove anything except numbers,letters,-,.\n",
    "    text=re.sub(r'\\s+',' ',text)    #remove extra white spaces\n",
    "    return text #return processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#scrape wikipedia page using Beautiful Soup, extract paragraphs and tokenize them to sentences\n",
    "response=urllib.request.urlopen('https://en.wikipedia.org/wiki/R2-D2')\n",
    "html=response.read()\n",
    "soup=BeautifulSoup(html,'lxml')\n",
    "para=soup.find_all('p') #extracting paragraphs\n",
    "text=\"\"\n",
    "for p in para:\n",
    "    text+=p.text\n",
    "# print(text)\n",
    "text=preprocess(text)\n",
    "sent_list=nltk.sent_tokenize(text) #tokenize text to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize sentences to individual words of sentences and also remove stopwords\n",
    "words1=[nltk.word_tokenize(sent.lower()) for sent in sent_list]\n",
    "for i in range(len(words1)):  \n",
    "    words1[i] = [w for w in words1[i] if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Doc2vec to create a vector from the \n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(words1)]\n",
    "model = Doc2Vec(vector_size=100, window=10, min_count=1, workers=4)\n",
    "model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the modell with 100 epochs\n",
    "max_epochs=100\n",
    "for epoch in range(max_epochs):\n",
    "#     print('iteration {0}'.format(epoch))\n",
    "    model.train(documents,total_examples=model.corpus_count,epochs=model.epochs)\n",
    "    model.alpha -= 0.0002 # decrease the learning rate\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#greet the user\n",
    "greeting_input_texts = (\"hi\",\"hey\", \"heys\", \"hello\", \"morning\", \"evening\",\"greetings\",)\n",
    "greeting_replie_texts = [\"hey\", \"hey hows you?\", \"*nods*\", \"hello there\", \"ello\", \"Welcome, how are you\"]\n",
    " \n",
    "def reply_greeting(text):\n",
    "    for word in text.split():\n",
    "        if word.lower() in greeting_input_texts:\n",
    "            return random.choice(greeting_replie_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give reply function that takes user input and returns required most similar sentence and the similarity number\n",
    "def give_reply(user_input):\n",
    "    chatbot_response=''\n",
    "    user_input=preprocess(user_input) #preprocess user input\n",
    "    user_sent=nltk.word_tokenize(user_input.lower()) #tokenize user sentence\n",
    "    input_vector=model.infer_vector(user_sent) #infer the vector from the trained model\n",
    "    sims = model.docvecs.most_similar([input_vector], topn=len(model.docvecs))\n",
    "    sent_index,similarity=sims[0][0],sims[0][1] #extract most similar sentence and its similarity\n",
    "    return (sent_list[sent_index],similarity*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I will answer your queries regarding R2-D2:\n",
      "hey\n",
      "Chatbot: *nods*\n",
      "When was R2-D2 inducted into the Robot Hall of Fame?\n",
      "Chatbot: ('R2-D2 was inducted into the Robot Hall of Fame in 2003.', 49.33385252952576)\n",
      "bye\n",
      "Chatbot: Most welcome\n",
      "Chatbot: Take care, bye ..\n"
     ]
    }
   ],
   "source": [
    "#sample \n",
    "output_greetings=[\"thanks\",\"thank you very much\",\"thank you\",\"bye\"]\n",
    "print(\"Hello, I will answer your queries regarding R2-D2:\") #greet user\n",
    "continue_discussion=True\n",
    "while continue_discussion==True:\n",
    "    user_input = input() #get user input\n",
    "    user_input = user_input .lower()\n",
    "    if user_input in output_greetings or user_input =='':\n",
    "            continue_discussion=False\n",
    "            print(\"Chatbot: Most welcome\")\n",
    "    elif reply_greeting(user_input)!=None:\n",
    "                print(\"Chatbot: \"+reply_greeting(user_input))\n",
    "    else:\n",
    "        print(\"Chatbot: \",end=\"\")\n",
    "        print(give_reply(user_input))\n",
    "print(\"Chatbot: Take care, bye ..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
